{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Another powerful and widely used learning algorithm is the Support Vector Machine (SVM), which can be considered an extension of the perceptron. In SVMs the optimization objective is to maximize the margin instead of minimizing the misclassification errors. However, in SVMs our optimization objective is to maximize the margin. The margin is defined as the distance between the separating hyperplane (decision boundary) and the training samples that are closest to this hyperplane, which are the so-called **support vectors**. \n",
    "\n",
    "<img src=\"images/support_vectors.jpg\" alt=\"Support Vectors\" title=\"Support Vectors\" height=\"350\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Margin Intuition\n",
    "\n",
    "To get an idea of the margin maximization, let's take a closer look at those positive and negative hyperplanes that are parallel to the decision boundary, which can be expressed as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{matrix}\n",
    "w_0 + w^Tx_{pos} = 1 & (1) \\\\\n",
    "w_0 + w^Tx_{neg} = -1 & (2)\n",
    "\\end{matrix}\n",
    "\\end{equation*}\n",
    "\n",
    "If we subtract those two linear equations (1) and (2) from each other, we get:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Rightarrow w^T \\left( x_{pos} - x_{neg} \\right) = 2\n",
    "\\end{equation*}\n",
    "\n",
    "We can normalize this equation by the length of the vector $w$, which is defined as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\| w \\| = \\sqrt{\\sum_{j=1}^m w_j^2}\n",
    "\\end{equation*}\n",
    "\n",
    "So we arrive at the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{w^T \\left( x_{pos} - x_{neg} \\right) }{\\| w \\|} = \\frac{2}{\\| w \\|}\n",
    "\\end{equation*}\n",
    "\n",
    "The left side of the preceding equation can then be interpreted as the distance between the positive and negative hyperplane, which is the so-called margin that we want to maximize.\n",
    "\n",
    "Now, the objective function of the SVM becomes the maximization of this margin by maximizing $\\frac{2}{\\| w \\|}$ under the constraint that the samples are classified correctly, which can be written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{matrix}\n",
    "w_0 + w^T x^{(i)} \\geq 1 & \\text{ if } y^{(i)}= 1 \\\\\n",
    "w_0 + w^T x^{(i)} \\leq -1 & \\text{ if } y^{(i)} = -1 \\\\\n",
    "\\text{for } i = 1 \\dots N & \\\\\n",
    "\\end{matrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, N is the number of samples in our dataset.\n",
    "\n",
    "These two equations basically say that all negative samples should fall on one side of the negative hyperplane, whereas all the positive samples should fall behind the positive hyperplane, which can also be written more compactly as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "y^{(i)} \\left( w_0 + w^T x^{(i)} \\right) \\geq 1 \\forall_i\n",
    "\\end{equation*}\n",
    "\n",
    "In practice though, it is easier to minimize the reciprocal term $\\frac{1}{2} \\| w \\|^2$ , which can be solved by quadratic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with a nonlinearly separable case using slack variables\n",
    "\n",
    "The motivation for introducing the slack variable $\\xi$ was that the linear constraints need to be relaxed for nonlinearly separable data to allow the convergence of the optimization in the presence of misclassifications, under appropriate cost penalization.\n",
    "\n",
    "The positive-values slack variable is simply added to the linear constraints:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{matrix}\n",
    "w_0 + w^T x^{(i)} \\geq 1 - \\xi^{(i)} & \\text{if } y^{(i)} = 1 \\\\\n",
    "w_0 + w^T x^{(i)} \\leq -1 + \\xi^{(i)} & \\text{if } y^{(i)} = -1 \\\\\n",
    "\\text{for } i = 1 \\dots N\n",
    "\\end{matrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $N$ is the number of samples in our dataset. So the new objective to be minimized (subject to the constraints) becomes:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{2} \\| w \\|^2 + C \\left( \\sum_i \\xi^{(i)} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Via the variable $C$, we can then control the penalty for misclassification. Large values of $C$ correspond to large error penalties, whereas we are less strict about misclassification errors if we choose smaller values for $C$. We can then use the $C$ parameter to control the width of the margin and therefore tune the bias-variance trade-off, as illustrated in the following figure:\n",
    "\n",
    "<img src=\"images/c_values.jpg\" alt=\"Large and Small values for C param\" title=\"Large and Small values for C param\" height=\"200\" width=\"350\">\n",
    "\n",
    "This concept is related to regularization: where decreasing the value of C increases the bias and lowers the variance of the model.\n",
    "\n",
    "Now that we have learned the basic concepts behind a linear SVM, let us train an SVM model to classify the different flowers in our Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data los datos de iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando la clase SVC clasifique el dataset de iris\n",
    "# grafique el espacio de clasificaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes our datasets are too large to fit into computer memory. Thus, scikit-learn also offers alternative implementations via the SGDClassifier class, which also supports online learning via the partial_fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilice la clase SGDClassificer con el parametro loss='hinge' para emular una MSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear problems using a kernel SVM\n",
    "\n",
    "\n",
    "Another reason why SVMs enjoy high popularity among machine learning practitioners is that it can be easily kernelized to solve nonlinear classification problems. First create a sample dataset to see what such a nonlinear classification problem may look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea un dataset artificial XOR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind kernel methods to deal with such linearly inseparable data is to create nonlinear combinations of the original features to project them onto a higher-dimensional space via a mapping function $\\phi$ where it becomes linearly separable. we can transform a two-dimensional dataset onto a new three-dimensional feature space where the classes become separable via the following projection:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\phi \\left(x_1, x_2 \\right) = \\left( z_1, z_2, z_3 \\right) = \\left( x_1, x_2, x_1^2 + x_2^2 \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "<img src=\"images/svm_linear_projection.jpeg\" alt=\"SVM linear projection\" title=\"SVM linear projection\" height=\"400\" width=\"550\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the kernel trick to find separating hyperplanes in high-dimensional space\n",
    "\n",
    "To solve a nonlinear problem using an SVM, we would transform the training data onto a higher-dimensional feature space via a mapping function $\\phi$ and train a linear SVM model to classify the data in this new feature space. Then, we can use the same mapping function $\\phi$ to transform new, unseen data to classify it using the linear SVM model.\n",
    "\n",
    "However, this construction is computationally expensive in high-dimensional data. This is where the so-called kernel trick comes into play, in practice all we need is to replace the dot product $x^{(i)T}x^{(j)}$ by $\\phi \\left(x^{(i)} \\right)^T \\phi \\left(x^{(j)} \\right)$. In order to save the expensive step of calculating this dot product between two points explicitly, we define a so-called **kernel function**:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\kappa \\left( x^{(i)}, x^{(j)} \\right) = \\phi \\left( x^{(i)} \\right)^T \\phi \\left( x^{(j)} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "One of the most widely used kernels is the Radial Basis Function (RBF) kernel or simply called the Gaussian kernel:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\kappa \\left( x^{(i)}, x^{(j)} \\right) = exp \\left( - \\frac{\\| x^{(i)} - x^{(j)} \\|^2}{2\\sigma^2} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "This is often simplified to:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\kappa \\left( x^{(i)}, x^{(j)} \\right) = exp \\left( \\gamma \\| x^{(i)} - x^{(j)} \\|^2 \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $\\gamma = \\frac{1}{2\\sigma^2}$ is a free parameter that is to be optimized.\n",
    "\n",
    "Roughly speaking, the term kernel can be interpreted as a similarity function between a pair of samples. The minus sign inverts the distance measure into a similarity score, and, due to the exponential term, the resulting similarity score will fall into a range between 1 (for exactly similar samples) and 0 (for very dissimilar samples).\n",
    "\n",
    "\n",
    "Let's train a kernel SVM that draws a nonlinear decision boundary that separates the XOR data well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilice SVC(kernel='rbf', random_state=1, gamma=0.1, C=10.0) para realizar una clasificaci√≥n en un espacio\n",
    "# linealmente no separable\n",
    "\n",
    "# grafique el espacio de clasificaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\gamma$, which we set to gamma=0.1, can be understood as a cut-off parameter for the Gaussian sphere. If we increase the value for $\\gamma$, we increase the influence or reach of the training samples, which leads to a tighter and bumpier decision boundary. To get a better intuition for $\\gamma$, let us apply an RBF kernel SVM to our Iris flower dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilize el mismo concepto para clasificar el dataset de iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us increase the value of $\\gamma$ and observe the effect on the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifique el par√°metro de gramma y de C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
