{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Simple Machine Learning Algorithms for Classification\n",
    "\n",
    "In this chapter, we will make use of two of the first algorithmically described machine learning algorithms for classification, the perceptron and adaptive linear neurons. We will start by implementing a perceptron step by step in Python and training it to classify different flower species in the Iris dataset.\n",
    "\n",
    "## Artificial neurons\n",
    "Trying to understand how the biological brain works, in order to design AI, Warren McCulloch and Walter Pitts published the first concept of a simplified brain cell, the so-called McCulloch-Pitts (MCP) neuron, in 1943. Neurons are interconnected nerve cells in the brain that are involved in the processing and transmitting of chemical and electrical signals, which is illustrated in the following figure:\n",
    "\n",
    "<img src=\"images/neuron_cell.jpeg\" alt=\"Neuron cells\" title=\"Neuron Cells\" width=\"550\" height=\"350\">\n",
    "\n",
    "Only a few years later, Frank Rosenblatt published the first concept of the perceptron learning rule based on the MCP neuron model. With his perceptron rule, Rosenblatt proposed an algorithm that would automatically learn the optimal weight coefficients that are then multiplied with the input features in order to make the decision of whether a neuron fires or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The formal definition of an artificial neuron\n",
    "More formally, we can put the idea behind artificial neurons into the context of a binary classification task.  We can then define a decision function $(\\phi(z))$ that takes a linear combination of certain input values $x$ and a corresponding weight vector $w$, where $z$ is the so-called net input $z = w_1x_1 + \\dots + w_mx_m$:\n",
    "\n",
    "\\begin{equation*}\n",
    "w = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "\\vdots \\\\\n",
    "w_m\n",
    "\\end{bmatrix}, x = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_m\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "In the perceptron algorithm, the decision function $\\phi(\\cdot)$ is a variant of a **unit step function**:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\phi(z) = \\left\\{ \\begin{matrix}\n",
    "    1 & if z \\ge \\Theta \\\\\n",
    "    -1 & otherwise\n",
    "    \\end{matrix} \\right.\n",
    "\\end{equation*}\n",
    "\n",
    "For simplicity, we can bring the threshold $\\theta$ to the left side of the equation and define a weight-zero as $w_0 = -\\Theta$ and $x_0 = 1$ so that we write z in a more compact form:\n",
    "\n",
    "\\begin{equation*}\n",
    "z = w_0x_0 + w_1x_1 + \\dots + w_mx_m = w^T x\n",
    "\\end{equation*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\phi(z) = \\left\\{ \\begin{matrix}\n",
    "    1 & if z \\ge 0 \\\\\n",
    "    -1 & otherwise\n",
    "    \\end{matrix} \\right.\n",
    "\\end{equation*}\n",
    "\n",
    "In machine learning literature, the negative threshold, or weight, $w_0 = -\\Theta$ , is usually called the *bias unit*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure illustrates how the net input $z = w^T x$ is squashed into a binary output $(-1 \\text{ or } 1)$ by the decision function of the perceptron (left subfigure) and how it can be used to discriminate between two linearly separable classes (right subfigure):\n",
    "\n",
    " <img src=\"images/perceptron_binary_output.jpeg\" alt=\"Perceptron binary Output\" title=\"Perceptron binary Output\" width=\"550\" height=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron learning rule\n",
    "\n",
    "Rosenblatt's initial perceptron rule is fairly simple and can be summarized by the following steps:\n",
    "\n",
    "> 1. Initialize the weights to 0 or small random numbers.\n",
    "> 2. For each training sample $x^{(i)}$:\n",
    ">   1. Compute the output value $\\hat{y}$.\n",
    ">   2. Update the weights.\n",
    "\n",
    "Here, the output value is the class label predicted by the unit step function that we defined earlier, and the simultaneous update of each weight in the weight vector w can be more formally written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "w_j := w_j + \\Delta w_j\n",
    "\\end{equation*}\n",
    "\n",
    "The value of $\\Delta w_j$, which is used to update the weight $w_j$ , is calculated by the perceptron learning rule:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Delta w_j = \\eta \\left( y^{(i)} - \\hat{y}^{(i)} \\right)x^{(i)}_j\n",
    "\\end{equation*}\n",
    "\n",
    "Where $\\eta$ is the learning rate, $y^{(i)}$ is the **true class label** of the ith training sample, $\\hat{y}^{(i)}$ and is the **predicted class label**. For example, for a two-dimensional dataset, we would write the update as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Delta w_0 = \\eta\\left( y^{(i)} - output^{(i)} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Delta w_1 = \\eta \\left( y^{(i)} - output^{(i)} \\right) x_1^{(i)}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Delta w_2 = \\eta \\left( y^{(i)} - output^{(i)} \\right) x_2^{(i)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small.\n",
    "\n",
    "<img src=\"images/separation_spaces.jpeg\" alt=\"linear and non-linear separable spaces\" title=\"linear and non-linear separable spaces\" width=\"550\" height=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us summarize what we just learned in a simple diagram that illustrates the general concept of the perceptron:\n",
    "\n",
    "<img src=\"images/perceptron_diagram.jpeg\" alt=\"Perceptron\" title=\"Perceptron\" width=\"550\" height=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a Perceptron in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementa la clase perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a perceptron model\n",
    "\n",
    "To test our perceptron implementation, we will load the two flower classes Setosa and Versicolor from the Iris dataset. Although the perceptron rule is not restricted to two dimensions, we will only consider the two features sepal length and petal length for visualization purposes. Also, we only chose the two flower classes Setosa and Versicolor for practical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# carga los datos de iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraer los primeros 100 (50 para iris-setosa, 50 para iris-versicolor) y convertir etiquetas de clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# graficar los datos obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenar el perceptron con eta=0.1 y 10 epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafica el error de clasificación errónea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# una función para graficar resultados\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def plot_decision_regions(x, y, classifier, resolution=0.02):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "    x2_min, x2_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    z = z.reshape(xx1.shape)\n",
    "\n",
    "    plt.contourf(xx1, xx2, z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=x[y == cl, 0],\n",
    "                    y=x[y == cl, 1],\n",
    "                    alpha=0.8,\n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx],\n",
    "                    label=cl,\n",
    "                    edgecolors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliza la función anterior para graficar los espacio de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
