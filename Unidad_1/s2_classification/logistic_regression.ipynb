{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling class Probabilities via Logistic Regression\n",
    "\n",
    "\n",
    "## Logistic regression intuition and conditional probabilities\n",
    "\n",
    "Logistic regression is a classification model that is very easy to implement but performs very well on linearly separable classes. Logistic Regression is a linear model for binary classification that can be extended to multiclass classification, for example, via the OvR technique.\n",
    "\n",
    "To explain the idea behind logistic regression as a probabilistic model, let's first introduce the **odds ratio**: the odds in favor of a particular event. The odds ratio can be written as $\\frac{p}{1 - p}$ where $p$ stands for the probability of the positive event. The term positive event does not necessarily mean good, but refers to the event that we want to predict. We can then further define the *logit* function, which is simply the logarithm of the odds ratio (log-odds):\n",
    "\n",
    "\\begin{equation*}\n",
    "logit\\left( p\\right) = \\log \\frac{p}{\\left( 1 - p\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "The logit function takes as input values in the range 0 to 1 and transforms them to values over the entire real-number range, which we can use to express a linear relationship between feature values and the log-odds:\n",
    "\n",
    "\\begin{equation*}\n",
    "logit \\left( p \\left( y = 1|x \\right) \\right) = w_0x_0 + w_1x_1 + \\dots + w_mx_x = \\sum_{i=0}^m w_ix_i = w^T x\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $p \\left( y=1|x \\right)$ is the conditional probability that a particular sample belongs to class 1 given its features **x**.\n",
    "\n",
    "Now, we are actually interested in predicting the probability that a certain sample belongs to a particular class, which is the inverse form of the *logit* function. It is also called logistic sigmoid function, sometimes simply abbreviated to sigmoid function due to its characteristic S-shape:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\phi \\left( z \\right) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation*}\n",
    "\n",
    "Here z is the net input, the linear combination of weights and sample features,\n",
    "\n",
    "\\begin{equation*}\n",
    "z = w^T x = w_0x_0 + w_1x_1 + \\dots + w_mx_m\n",
    "\\end{equation*}\n",
    "\n",
    "Let's see the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementa la función sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafica la función de -7 a 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that this sigmoid function takes real number values as input and transforms them into values in the range $[0, 1]$ with an intercept at $\\phi \\left( z\\right) = 0.5$.\n",
    "\n",
    "In Adaline, we used the identity function $\\phi \\left( z \\right) = z$ as the activation function. In logistic regression, this activation function simply becomes the sigmoid function that we defined earlier. The difference between Adaline and logistic regression is illustrated in the following figure:\n",
    "\n",
    "<img src=\"images/adaline_lg_differences.jpg\" height=\"200\" width=\"350\">\n",
    "\n",
    "The output of the sigmoid function is then interpreted as the probability of a particular sample belonging to class 1, $\\phi \\left( z \\right) = P \\left( y = 1 | x; w \\right)$, given its features **x** parameterized by the weights **w**.\n",
    "\n",
    "The predicted probability can then simply be converted into a binary outcome via a threshold function:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y} = \\left\\{ \\begin{matrix}\n",
    "1 & \\text{if } \\phi \\left( z \\right) \\geq 0.5 \\\\\n",
    "0 & \\text{ otherwise}\n",
    "\\end{matrix} \\right.\n",
    "\\end{equation*}\n",
    "\n",
    "If we look at the preceding plot of the sigmoid function, this is equivalent to the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y} = \\left\\{ \\begin{matrix}\n",
    "1 & \\text{if } z \\geq 0.0 \\\\\n",
    "0 & \\text{ otherwise}\n",
    "\\end{matrix} \\right.\n",
    "\\end{equation*}\n",
    "\n",
    "The application of Logistic Regression includes weather forecasting and to predict diseases in pacients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the weights of the logistic cost function\n",
    "\n",
    "Let us briefly talk about how we fit the parameters of the model, for instance the weights **w**. Previously, we defined the sum-squared-error cost function as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "j \\left( w \\right) = \\sum_i \\frac{1}{2} \\left( \\phi \\left( z^{(i)} \\right) - y^{(i)} \\right)^2\n",
    "\\end{equation*}\n",
    "\n",
    "To explain how we can derive the cost function for logistic regression, let's first define the likelihood L that we want to maximize when we build a logistic regression model, assuming that the individual samples in our dataset are independent of one another. The formula is as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "L \\left( w \\right) = P \\left( y|x;w \\right) = \\prod_{i=1}^n P \\left( y^{(i)} | x^{(i)};w \\right) = \\prod_{i=1}^n \\left( \\phi \\left( z^{(i)} \\right) \\right)^{y^{(i)}} \\left( 1 - \\phi \\left( z^{(i)} \\right) \\right)^{1-y^{(i)}}\n",
    "\\end{equation*}\n",
    "\n",
    "In practice, it is easier to maximize the (natural) log of this equation, which is called the log-likelihood function:\n",
    "\n",
    "\\begin{equation*}\n",
    "l \\left( w \\right) = \\log{}L \\left( w \\right) = \\sum_{i=1}^n \\left[ y^{(i)} \\log{ \\left( \\phi \\left( z^{(i)} \\right) \\right)} + \\left( 1 - y^{(i)} \\right) \\log{\\left( 1 - \\phi \\left( z^{(i)} \\right) \\right)} \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "Firstly, applying the log function reduces the potential for numerical underflow, which can occur if the likelihoods are very small. Secondly, we can convert the product of factors into a summation of factors, which makes it easier to obtain the derivative of this function via the addition trick.\n",
    "\n",
    "Now we could use an optimization algorithm such as gradient ascent to maximize this log-likelihood function. Alternatively, let's rewrite the log-likelihood as a cost function J that can be minimized using gradient descent.\n",
    "\n",
    "\\begin{equation*}\n",
    "J \\left( w \\right) = \\sum_{i=1}^n \\left[ -y^{(i)} \\log{\\left( \\phi \\left( z^{(i)} \\right) \\right)} - \\left(1 - y^{(i)} \\right) \\log{\\left(1 - \\phi \\left( z^{(i)} \\right) \\right)} \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "To get a better grasp of this cost function, let us take a look at the cost that we calculate for one single-sample training instance:\n",
    "\n",
    "\\begin{equation*}\n",
    "J \\left( \\phi \\left( z \\right), y; w \\right) = -y \\log{\\left( \\phi \\left( z \\right) \\right)} - \\left( 1 - y \\right) \\log{\\left( 1 - \\phi \\left(z \\right) \\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "Looking at the equation, we can see that the first term becomes zero if $y = 0$, and the second term becomes zero if $y = 1$:\n",
    "\n",
    "\\begin{equation*}\n",
    "J \\left( \\phi \\left( z \\right), y; w \\right) = \\left\\{\n",
    "\\begin{matrix}\n",
    "-\\log{\\left( \\phi \\left(z \\right) \\right)} & \\text{ if } y = 1 \\\\\n",
    "-\\log{\\left( 1 -  \\phi \\left(z \\right) \\right)} & \\text{ if } y = 0 \\\\\n",
    "\\end{matrix}\n",
    "\\right.\n",
    "\\end{equation*}\n",
    "\n",
    "Let's write a short code snippet to create a plot that illustrates the cost of classifying a single-sample instance for different values of $\\phi \\left( z \\right)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define las funciones cost_1 y cost_0, pruebalas en el rango -10 a 10 con intervalos de 0.1\n",
    "# grafica su salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the cost approaches 0 (continuous line) if we correctly predict that a sample belongs to class 1. Similarly, we can see on the y-axis that the cost also approaches 0 if we correctly predict $y = 0$ (dashed line). However, if the prediction is wrong, the cost goes towards infinity. The main point is that we penalize wrong predictions with an increasingly larger cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting an Adaline implementation into an algorithm for logistic regression\n",
    "\n",
    "We could simply substitute the cost function J in our Adaline implementation, with the new cost function:\n",
    "\n",
    "\\begin{equation*}\n",
    "J \\left( w \\right) = - \\sum_i y^{(i)} \\log{\\left( \\phi \\left( z^{(i)} \\right) \\right)} + \\left(1 - y^{(i)} \\right) \\log{\\left( 1 - \\phi \\left( z^{(i)} \\right) \\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "We use this to compute the cost of classifying all training samples per epoch. Also, we need to swap the linear activation function with the sigmoid activation and change the threshold function to return class labels 0 and 1 instead of -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obten y prepara los datos de iris con el cuadernos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# implementa la clase LogisticRegressionGS\n",
    "# entrana una neurona LogisticRegressionGD con 1000 epocas, eta=0.05 y random_state=1\n",
    "# grafica los espacios de búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression using with scikit-learn\n",
    "\n",
    "let's learn how to use scikit-learn's more optimized implementation of logistic regression that also supports multi-class settings off the shelf (OvR by default). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliza las clases StandardScaler y LogisticRegression para la clasificación de las 3 flores del dataset de iris.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# prepare data\n",
    "\n",
    "# train a model\n",
    "\n",
    "# After fitting the model on the training data, we plotted the decision regions, training samples, and test samples,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"What is this mysterious parameter C?\". Before answer it, et's finish our discussion of class-membership probabilities. The probability that training examples belong to a certain class can be computed using the predict_proba method. For example, we can predict the probabilities of the first three samples in the test set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr.predict_proba(X_test_std[:3, :])\n",
    "# lr.predict_proba(X_test_std[:3, :]).sum(axis=1) # to confirm the sum\n",
    "lr.predict_proba(X_test_std[:3, :]).argmax(axis=1)  # predict the class by the largest column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X_test_std[:3, :]) # the same, but using predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning: if you want to predict the class label of a single flower sample: scikit-learn expects a two-dimensional array as data input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tackling overfitting via regularization\n",
    "\n",
    "Overfitting is a common problem in machine learning, where a model performs well on training data but does not generalize well to unseen data (test data). If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters that lead to a model that is too complex given the underlying data. Similarly, our model can also suffer from underfitting (high bias), which means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance on unseen data.\n",
    "\n",
    "Although we have only encountered linear models for classification so far, the problem of overfitting and underfitting can be best illustrated by comparing a linear decision boundary to more complex, nonlinear decision boundaries as shown in the following figure:\n",
    "\n",
    "<img src=\"images/overfitting_underfitting.jpg\" height=\"200\" width=\"350\">\n",
    "\n",
    "One way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter (weight) values. The most common form of regularization is so-called L2 regularization (sometimes also called L2 shrinkage or weight decay), which can be written as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\lambda}{2} \\| w \\| = \\frac{\\lambda}{2} \\sum_{j=1}^m w_j^2\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $\\lambda$ is the so-called regularization parameter.\n",
    "\n",
    "The cost function for logistic regression can be regularized by adding a simple regularization term, which will shrink the weights during model training:\n",
    "\n",
    "\\begin{equation*}\n",
    "J \\left( w \\right) = \\sum_{i=1}^n \\left[ -y^{(i)} \\log{\\left( \\phi \\left( z^{(i)} \\right) \\right)} - \\left( 1 - y^{(i)} \\right) \\log{\\left( 1 - \\phi \\left( z^{(i)} \\right) \\right)} \\right] + \\frac{\\lambda}{2} \\| w \\|^2\n",
    "\\end{equation*}\n",
    "\n",
    "Via the regularization parameter $\\lambda$, we can then control how well we fit the training data while keeping the weights small. By increasing the value of $\\lambda$, we increase the regularization strength.\n",
    "\n",
    "The term C is directly related to the regularization parameter , which is its inverse. Consequently, decreasing the value of the inverse regularization parameter C means that we are increasing the regularization strength, which we can visualize by plotting the L2-regularization path for the two weight coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genera una gráfica del comportamiento del parámetro de penelización (C) de 1x10^-5 y 1x10^5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fitted ten logistic regression models with different values for the inverse-regularization parameter C. we only collected the weight coefficients of class 1 versus all classifiers. As we can see, the weight coefficients shrink if we decrease parameter C, that is, if we increase the regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
