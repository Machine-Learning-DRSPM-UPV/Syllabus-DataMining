{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbors\n",
    "\n",
    "KNN is a typical example of a lazy learner. It is called lazy not because of its apparent simplicity, but because it doesn't learn a discriminative function from the training data, but memorizes the training dataset instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric versus nonparametric models\n",
    "\n",
    "Machine learning algorithms can be grouped into parametric and nonparametric models. Using parametric models, we estimate parameters from the training dataset to learn a function that can classify new data points without requiring the original training dataset anymore. Typical examples of parametric models are the perceptron, logistic regression, and the linear SVM. In contrast, nonparametric models can't be characterized by a fixed set of parameters, and the number of parameters grows with the training data. Two examples of non-parametric models that we have seen so far are the decision tree classifier/random forest and the kernel SVM. KNN belongs to a subcategory of instance-based non-parametric models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN algorithm itself is fairly straightforward and can be summarized by the following steps:\n",
    "> 1. Choose the number of k and a distance metric.\n",
    "> 2. Find the k-nearest neighbors of the sample that we want to classify.\n",
    "> 3. Assign the class label by majority vote.\n",
    "\n",
    "The following figure illustrates how a new data point (?) is assigned the triangle class label based on majority voting among its five nearest neighbors.\n",
    "\n",
    "<img src=\"images/knn_example.jpeg\" alt=\"KNN example\" title=\"KNN example\" height=\"200\" width=\"350\">\n",
    "\n",
    "Based on the chosen distance metric, the KNN algorithm finds the k samples in the training dataset that are closest (most similar) to the point that we want to classify. The class label of the new data point is then determined by a majority vote among its k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare el dataset de iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilice la clase de KNeighborsClassifier para clasificar el dataset de iris\n",
    "# grafique el espacio de clasificaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right choice of k is crucial to find a good balance between overfitting and underfitting. We also have to make sure that we choose a distance metric that is appropriate for the features in the dataset. The minkowski distance is a generalization of the Euclidean and Manhattan distance, which can be written as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "d \\left( x^(i), x^{(j)} \\right) = \\sqrt[p]{\\sum_k \\left|x_k^{(i)} - x_k^{(j)} \\right|^p}\n",
    "\\end{equation*}\n",
    "\n",
    "It becomes the Euclidean distance if we set the parameter p=2 or the Manhattan distance at p=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The curse of dimensionality\n",
    "\n",
    "\n",
    "It is important to mention that KNN is very susceptible to overfitting due to the curse of dimensionality. The curse of dimensionality describes the phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset. Intuitively, we can think of even the closest neighbors being too far away in a high-dimensional space to give a good estimate.\n",
    "\n",
    "We have discussed the concept of regularization in the section about logistic regression as one way to avoid overfitting. However, in models where regularization is not applicable, such as decision trees and KNN, we can use feature selection and dimensionality reduction techniques to help us avoid the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
