{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Challenges of Machine Learning\n",
    "\n",
    "In short, since your main task is to select a learning algorithm and train it on some data, the two things that can go wrong are **\"bad algorithm\"** and **\"bad data\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insufficient Quantity of Training Data\n",
    "\n",
    "Machine learning algorithms take a lot of data for most tasks and work properly. Even for very simple problems you typically need thousands of examples, and for complex problems such as image or speech recognition you may need millions of examples (unles you can reuse parts of an existing model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonrepresentative Training Data\n",
    "\n",
    "In order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to. This is true whether you use instance-based learning or model-based learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# to plot pretty figures\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"fundamentals\"\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "datapath = os.path.join(\"datasets\", \"lifesat\", \"\")\n",
    "\n",
    "oecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\n",
    "oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "gdp_per_capita = pd.read_csv(datapath+\"gdp_per_capita.csv\", thousands=',', delimiter='\\t',\n",
    "                             encoding='latin1', na_values=\"n/a\")\n",
    "gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n",
    "gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita, left_index=True, right_index=True)\n",
    "full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n",
    "remove_indices = [0, 1, 6, 8, 33, 34, 35]\n",
    "keep_indices = list(set(range(36)) - set(remove_indices))\n",
    "\n",
    "sample_data = full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\n",
    "missing_data = full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[remove_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos el data frame missing_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se realize lo siguiente:\n",
    "# 1.- mostrar una gráfica de disperción los datos originales\n",
    "# 2.-  agregamos los datos faltantes. Los paises: Brazil, Mexico, Chile, Czech Republic, Switzerland, Luxemburg\n",
    "# 3.- Generamos un  modelo lineal con los datos originales\n",
    "# 4.- Generamos un modelo con los datos faltantes\n",
    "# 5.- Mostramos las rectas definidas por los coeficientes de cada modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a nonrepresentative training set, we trained a model that is unlikely to make accurate predictions, especially for very poor and very rich countries.\n",
    "\n",
    "Is is crucial to use a training set that is representative of the cases you want to generalize to. This is often harder that it sounds: if the sample is too small, you will have *sampling noise* (e.e. nonrepresentative data as a result of chance), but even very large samples can be nonrepresentative if the sampling method is flawed. This  is called *sampling bias*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poor-Quality Data\n",
    "\n",
    "If your training data is full of errors, outliers, and noise (e.g., due to poor-quality measurements), it will make it harder for the system to detect the underlying patterns, so your system is less likely to perform well. It is often well worth the effort to spend time cleaning up your training data. The truth is, most data scientists spend a significant part of their time doing just that. For example:\n",
    "\n",
    "* If some instances are clearly outliers, it may help to simpy discard them or try to fix errors manually.\n",
    "* If some instances are missing of few features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irrelevant Features\n",
    "\n",
    "As the saying goes: garbage in, garbage out. Your system will only be capable of learning if the training data contains enough relevant features and not too many irrelevant ones. A critical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called *feature engineering*, involves:\n",
    "\n",
    "* *Feature selection*: selecting the most useful feature to train on among existing features.\n",
    "* *Feature extraction*: combining existing features to produce a more useful one (*dimensionality reduction*).\n",
    "* Creating new features by gathrering new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting the training Data\n",
    "\n",
    "In Machine Learning *overfitting* means that the model performs well on the training data, but it does not generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un modelos polinomial utilizando: sklearn.preprocessing.PolynomialFeatures\n",
    "# estandarizamos los datos mediante preprocessing.StandardScaler()\n",
    "# Generamos un modelo utilizando LinearRegression().\n",
    "# Probamos: polinomios de grados: 15, 10, 5, y 2. (en ese orden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting happens when the model is too complex relative to the amount and noisiness of the training data. The possible solutions are:\n",
    "\n",
    "* To simplify the model by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polinomial model), by reducing the number of attributes in the training data or by constraining the model.\n",
    "* To gahter more training data\n",
    "* To reduce the noise in the training data (e.g., fix data errors and remove outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraining a model to make it simpler and reduce the risk of overfitting is called *regularization*. The amount of regularization to apply during learning can be controlled by a *hyperparameter*. A *hyperparameter* is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; the learning algorihtm will almos certainly not overfit the training data, but it will be less likely to find a good solution. Tuning hyperparameters is an important part of building a Machine Learning system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se realiza lo siguiente:\n",
    "# mostamos los datos originales y los datos faltantes en la misma gráfica en diferentes conjuntos (colores)\n",
    "# Se generarán 3 modelos lineales: datos originales (parciales), datos completos y datos regularizados.\n",
    "# se mostraŕan las rectas que definen esos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting the training data\n",
    "\n",
    "*Underfitting* is the opposite of overfitting; it ocurrs when your model is to simple to learn the underlying structure of the data.\n",
    "The main options to fix this problem are:\n",
    "* Selecting a more powerful model, with more parameters\n",
    "* Feeding better features to the learning algorithm (feature engineering)\n",
    "* Reducing the constraints on the model (e.g., reducing the regularization hyperparameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepping back\n",
    "\n",
    "* Machine Learning is about machines get better at some task by learning from data, instead of having to explicity code rules.\n",
    "* There are many different types of ML systems: supervised or not, batch or online, instance-based or model-based, and so on.\n",
    "* In a ML project you gather data in a training set, and you feed the training set to a learning algorithm. If the algorithm is model-based it tunes some parameters to fit the model to the training set (i.e., make good predictions on the training set itself), and then hopefully it will be able to make good predictions on new cases as well. If the algorithm is instance-based, it just learns the examples by heart and uses a similarity measure to generalize to new instances.\n",
    "* The system will not perform well if your training set is too small, or if the data is not representative, noisy, or polluted with irrelevant features (garbage in, garbage out). Lastly, your model needs to be  neither too simple (in which case it will underfit) nor too complex (in which case it will overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Validating\n",
    "\n",
    "The only way to know how well a model generalize to new cases is to actually try it out on new cases. The best option is to split your data into two sets: the *training set* and the *test set*.\n",
    "The error rate on noew cases is called the *generalization error* (or *out-of-sample error*), and by evaluating your model on the test set, you get an estimation of this error.\n",
    "\n",
    "> It is common to use 80% of the data for training and *hold out* 20% for testing.\n",
    "\n",
    "A common solution to the problem where you measure the generalization error multiple times on the test set, is having a set called *validation set*.\n",
    "\n",
    "To avoid \"wasting\" too much training data in validation set, a common technique is to use *cross-validation*: the training set is split into complementary subsets, and each model is trained against a different combination of these and validated against the remainin parts. One the model type and hyperparameters have been selected, a final model is trained using these hyperparameters on the full training set, and the generalized error is measured on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
