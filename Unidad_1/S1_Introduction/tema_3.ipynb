{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Machine Learning Project.\n",
    "\n",
    "The goal is just to ilulustrate the main steps of a Machine Learning project, pretending to be a recently hired data scientist in a real estate company. The main steps you will go through:\n",
    "\n",
    "1. Look at the big picture.\n",
    "2. Get the data.\n",
    "3. Discover and visualize the data to gain insights.\n",
    "4. Prepare the data for Machine Learning algorithms.\n",
    "5. Select a model and train it.\n",
    "6. Fine-tune your model.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor and maintain your system.\n",
    "\n",
    "\n",
    "## Working with Real Data.\n",
    "Here are a few places you can look to get data:\n",
    "\n",
    "* Popular open data repositories:\n",
    "  * [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n",
    "  * [Kaggle datasets](https://www.kaggle.com/datasets)\n",
    "  * [Amazon’s AWS datasets](http://aws.amazon.com/fr/datasets/)\n",
    "* Meta portals (they list open data repositories):\n",
    "  * http://dataportals.org/\n",
    "  * http://opendatamonitor.eu/\n",
    "  * http://quandl.com/\n",
    "* Other pages listing many popular open data repositories:\n",
    "  * [Wikipedia’s list of Machine Learning datasets](https://goo.gl/SJHN2k)\n",
    "  * [Quora.com question](http://goo.gl/zDR78y)\n",
    "  * [Datasets subreddit](https://www.reddit.com/r/datasets)\n",
    "  \n",
    "In this example we chose the California Housing Price dataset from the StatLib repository. This data set was based on data from the 1990 California census. We also added a categorical attribute and removed a rew features for teaching purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the big picture\n",
    "\n",
    "The problem is to build a model of housing prices in California using the dataset mentioned above. This data has metrics such as the population, median income, median housing price, and so on for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will just call them “districts” for short. **Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.**\n",
    "\n",
    "\n",
    "#### Frame the problem\n",
    "\n",
    "<img src=\"./img/ml_pipeline.png\" alt=\"ml pipeline\" title=\"A Machne Learning pipeline for real state investments.\" width=\"470\" height=\"330\" />\n",
    "<center><strong>Figure 1.-</strong> A Machne Learning pipeline for real state investments. </center>\n",
    "\n",
    "After get a look to the problem it is clearly a typical supervised learning task since you are given labeled training examples (each instance comes with the expected output, i.e., the district’s median housing price). It is also a typical regression task, since you are asked to predict a value. More specifically, this is a multivariate regression problem since the system will use multiple features to make a prediction (it will use the district’s population, the median income, etc.). Finally, there is no continuous flow of data coming in the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a Performance Measure\n",
    "A typical performance measure for regression problems is the Root Mean Square Error (RMSE). In measures the *standard deviation* of the errors the system makes in its predictions.\n",
    "\n",
    "\\begin{equation*}\n",
    "RMSE(X, h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^m (h(x^{(i)}) - y^{(i)})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "Even though the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. For example, suppose that there are many outlier districts. In that case, you may consider using the Mean Absolute Error\n",
    "\n",
    "\\begin{equation*}\n",
    "MAE(X, h) = \\frac{1}{m} \\sum_{i=1}^m | h(x^{(i)}) - yx^{(i)}|\n",
    "\\end{equation*}\n",
    "\n",
    "Both the RMSE and the MAE are ways to measure the distance between two vectors: the vector of\n",
    "predictions and the vector of target values. Various distance measures, or *norms*, are possible:\n",
    "\n",
    "* Computing the root of a sum of squares (RMSE) corresponds to the Euclidian norm: it is the notion of distance you are familiar with. It is also called the $\\ell_2$ norm, noted $\\| \\cdot \\|_2$ (or just $\\| \\cdot \\|$).\n",
    "* Computing the sum of absolutes (MAE) corresponds to the $\\ell_1$ norm, noted $\\| \\cdot \\|_1$. It is sometimes called the *Manhattan norm* because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.\n",
    "* Molre generally, the $\\ell_k$ *norm* of a vector **v** containing $n$ elements is defined as $\\|v\\|_k = (|v_0|^k+|v_1|^k + \\cdots + |v_n|^k)^{\\frac{1}{k}}$. $\\ell_0$ just jives the number of non-zero elements in the vector, and $\\ell_\\infty$ gives the maximum absolute value in the vector.\n",
    "* The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se utiliza la función fetch_housing_data para obtener los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observamos las estructura de los datos obtenidos. Se utiliza la función load_housing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la descripción de los datos utilizando housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos las categorias (ocean proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()  # Mostramos la descripción de los datos numericos (housing.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos una matriz con los histogramas de cada columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Test Set\n",
    "If you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of ML model. When you estimate the generalization error using the test set, your estimate will be too spotimistic and you will launch a system that will not perform well as expected. This is called *data snooping bias*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generamos una función que nos permita dividir los datos en test y train.\n",
    "def split_train_test(data, test_ratio):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos la función split_train_test con un 0.2, es decir un 20% para test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, but it is no perfect: if you run the program again, it will generate different test set! Over time, your ML algorithm will get to see the whole dataset, which is what you want to avoid.\n",
    "\n",
    "A common solution is to use each instance's identifier to decide wheter or not it should go in the test set. For example, you could compute a hash of each instance's identifier, keep only the last byte of the hash, and put the instance in the test set if this value is lower or equal to 51 (~20% of 256)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "# se generan dos funciones: test_set_check y split_train_test_by_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los datos de housing no tienen un identificador de columna, agregamos un índice\n",
    "# utilizamos la función de split_train_test_by_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o utilice las columnas más \"estables\" para crear un identificador único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos los datos con head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se utiliza la función sklearn.model_selection.train_test_split. Y mostramos el resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have considered pyrely random samplig methods. This is generally fine if your dataset is large enough (especially relative to the number of attributes), but if it is not, you run the risk of introducing a significant sampling bias. A *stratified sampling* is when the data to be sampled is divided into homogeneous subgrupos called *strata*, and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall population.\n",
    "\n",
    "Suppose you chatted with experts who told you that the median income is a very important attribute to predict median housing prices. Let’s look at the median income histogram more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostarmos el historial de median_income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have a sufficient number of instances in your dataset for each\n",
    "stratum, or else the estimate of the stratum’s importance may be biased. This means that you should not\n",
    "have too many strata, and each stratum should be large enough. The following code creates an income category attribute by dividing the median income by 1.5 (to limit the number of income categories), and rounding up using ceil (to have discrete categories), and then merging all the categories greater than 5 into category 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos por 1.5 para limitar el número de categorias de ingreso\n",
    "\n",
    "# Etiquete los que están arriba de 5 como 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos value_counts para contar los elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos el historial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un código para generar conjuntos etratificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contamos los elementos divididos entre el conjunto estratificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entre el total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# compararamos las proporciones de las categorías de ingresos en el conjunto de datos general.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elimine el atributo income_cat. Los datos vuelven a su estado original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover and Visualize the Data to Gain insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos una nueva copia de strat_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una gráfica de disperción utilizando la longitud y la latitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejoramos la visualización del mapa utilizando un alpha de 0.1\n",
    "# Se mostraŕa que las zonas mas obscuras son la aglomeración de las viviendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# echemos un vistazo a los precios de la vivienda. El radio de los círculos representa la población\n",
    "# del distrito y coloreamos el precio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos los datos sobre el mapa de california, con todo lo anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for correlations\n",
    "\n",
    "Since the dataset is not too large, you can easily compute the *standard correlation coefficient* (also called *Pearson's r*) between every pair of attibutes using the corr() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos la matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordenamos la correlación de mayor a menor de acuerdo a \"median_house_value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observamos la correlación de atributos utilizando *scatter_matrix* de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El atributo más prometedor para predecir el valor medio de la vivienda es el ingreso medio.\n",
    "# Observamos su grafica de correlación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with attribute combinations\n",
    "One last this you may want to do before actually preparing the data for Machine learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. Let’s create these new attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos nuevos datos utilizando los actuales.\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrroms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volvemos a generar la matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observamos la grafica de correlación de rooms_per_household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos el comando describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for Machine Learning algorithms\n",
    "It's time to prepare the data for Machine Learning Algorithms. The reasons for doing that are:\n",
    "* This will allow you to reproduce thise transformations easily on any dataset (e.g, the next time you get a fresh dataset).\n",
    "* You will  gradually build a library of transformation functions that you can reuse in future projects.\n",
    "* You can use these functions in your live systems to transform the new data before feeding it to your algorithms.\n",
    "* This will make it possible for you to easily try various transformations and see which combination of transformations works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos los datos.\n",
    "# separamos las características de sus etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "\n",
    "Most ML algorithms cannot work with missing features, so lest's case of them. *total_bedrooms* feature has some missing values, for fixing we have three options:\n",
    "\n",
    "* Get rid of the corresponding districts.\n",
    "* Get rid of the whole attribute\n",
    "* Set the values to some value (zero, the main, the median, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos las muestras que tengan datos incompletos (null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción 1: Eliminamos las muestras que tengan datos incompletos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción 2: eliminamos las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción 3: Reemplazamos los valos null por los valores medios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scikit-Learn* provides a handy class to take care of missing values: *Imputer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos utilizar SampleImputer para rellenar los datos incompletos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the median can only be computed on numerical attributes, we nedd to create a copy of the dta without the text attribute *ocean_proximity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quitamos ocean_proximity dado que no es un atributo numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos el imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos el atributo de statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checamos manualmente los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos el imputer para transformar el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un dataframe con a partir de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos las 5 primeras muestras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling text and Categorical Attributes\n",
    "Early we left out the categorical attribute *ocean_proximity* because it is a text attribute so we cannot compute its median. Most ML algorithms prefer to work with numbers anyway, so let's convert these categories from text to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos una variable housing_cat a aprtir de la columna \"ocean_proximity\"\n",
    "# mostramos las primeras 10 muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos sklearn.preprocesing.OrdinalEncoder para obtener etiquetas numéricas a partir de los datos no ordinales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observamos las categorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better: *housing_categories* is now purely numerical. *ordinal encoder* retuns the list of categories.\n",
    "\n",
    "One issue with this representation is that ML algorithms will assume that two nearby values are more similar that two distant values. To fix this issue, a comkmon solution is to create one binary attribute per category, This is called *one-hot encoding*, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos sklearn.preprocessing.OneHotEncoder para obtener ohe de los atributos categoricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos la matriz densa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply both transformation (from text categories to integer, then integer to one-hot vectors) using the *OneHotEncoder* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizamos el mismo proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos la matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos la matriz densa,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos las categorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If a categorical attribute has a large number of possible categories, then one-hot encoding will result in a large number of input features. This may slow down training and degraded performance. If this happens, you will want to produce denser representations. If this happends, you will want to produce denser represenations called *embeddings*, but this requires a good understanding of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom transformers\n",
    "\n",
    "Although Scikit-Learn provides may useful transformers, you will need to write your own for task such as custom cleanup operations or combining specific atribbutes. Remenber, Scikit-Learns relies on duck typing, you need to create a class and implement three methods: *fit()* (returning *self*, *transform()*, and *fit_transform()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos una clase propia para ralizar la combinación de los atributos\n",
    "# from combined_attributes_adder import CombinedAttributesAdder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos el data frame con los datos extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation Pipelines\n",
    "\n",
    "There are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the *pipeline* class to help with such sequences of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos un pipeline con imputer, combinación de atributos y standarscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos los datos numericos obtenidos del pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it would be nice if we could feed a Pandas DataFrame containing non-numerical columns directly into our pipeline, insted of having to first manually extract the numerical columns into a Numpy array. There is nothing in Scikit-Learn to handle Pandas DataFrames, but we can write a custom transformer for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agregamos al pipeline la transformación de los atributos categoricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos los datos preparados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos las dimensiones del data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and Train a Model\n",
    "\n",
    "#### Training and Evaluating on theTraining Set\n",
    "Les's train a **Linear Regression** model, like previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos un modelo lineal (sklearn.linear_model.LinearRegression) para generar un modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!. You now have a working Linear Regression model. Let's try it out on a few instances from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizamos predicciones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, but it's not perfect. The first prediction is off by close to 40%. Let's meausre this model on the whole training set using Scikit-Learn's mean_squared_error function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos los datos esperados vs los datos obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizando sklearn.metrics.mean_squared_error obtenemos el rmse de las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos mean_absolute_error para obtener el mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is an example of a model underfitting the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough.\n",
    " \n",
    " Let's train a *DecisionTreeRegressior*. This is a powerful model, capable of finding complex nonlinear relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos un nuevo modelo utilizando sklearn.tree.DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos las predicciones con el nuevo modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Evaluation Using Cross-Validation\n",
    "\n",
    " We use Scikit-Learn's *cross-validation* feature. The following code performs *K-fold cross-validation*: it randomly splits the training set into 10 distinct subset called *folds*, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos sklearn.model_selection.cross_val_score para evaluar el modelo mediente cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Scikit-Learn cross-validation features expect a utility function rather than a cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función que nos permita mostar las metricas de evaluación\n",
    "def display_scores(scores):\n",
    "    pass\n",
    "\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos validación cruzada para evaluar el modelo lineal, generado anteriormente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Decision Tree model is overfitting so badly that it performs worse than the Linear Regression Model.\n",
    "\n",
    "Let's try one last model: *RandomForestRegressor*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un nuevo modelo. Utilizamos sklearn.ensemble.RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos las predicciones y obtenemos el rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos validación cruzada para evaluar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest look promising. However, note that the score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set. Possible solution for overfitting are to simplify the model, constraint it (i.e., regularize it), or get a lot more training data.\n",
    "\n",
    "> for saving models that you experiment with can use the *pickle* module, or use sklean-externals.joblib.\n",
    "> ```python\n",
    ">  from sklearn.externals import joblib\n",
    ">  \n",
    ">  joblib.dump(my_model, \"my_model.pkl\") # save\n",
    ">  my_model_loaded = joblib.load(\"my_model.pkl\") # load\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune your model\n",
    "Let’s assume that you now have a shortlist of promising models. You now need to fine-tune them. \n",
    "\n",
    "#### Grid Search\n",
    "One way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of  hyperparameter values. This would be very tedious work and time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos validación cruzada para obtener el rmse de tree_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos sus scores mediante la función display_scores, previamente definida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volvemos a obtener los valroes del modelo lin_reg a partir de la validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos un nuevo modelo utilizando sklearn.ensamble.RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos su rmse\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos sus scores utilizando validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describimos los scores de los resultados obtenidos de la validación cruzada del modelo lin_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos GridSearchCV para obtener un modelo optimizado de sus hyperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos los mejores parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If *GridSearchCV is initialized with refit=True, then once it finds the best estimator using crossvalidation, it retrains it on the whole training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the score of each hyperparameter combination tested during the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos los resultados de cada modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Search\n",
    "The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter *search space* is large, it is often preferable to use *RandomizedSearchCV*. This approach has two main benefits:\n",
    "\n",
    "* If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few values per hyperparameter with the grid search approach).\n",
    "* You have more control over the computing budget you want to allocate to hyperparameter search, simply by setting the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos sklearn.model_selection.RandomizedSearchCV para una búsqueda aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos los resultados de cada modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the Best models and their errors\n",
    "You will often gain good insights on the problem by inspecting the best models. For example, the\n",
    "RandomForestRegressor can indicate the relative importance of each attribute for making accurate\n",
    "predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos la importancia de las características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos la importancia y sus nombres, de forma ordenada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whith this information, you may want to try dropping some of the less useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Your system on the test set\n",
    "Now is the time to evaluate the final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluamos el modelo final vs el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
