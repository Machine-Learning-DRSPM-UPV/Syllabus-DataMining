{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning a dataset\n",
    "\n",
    "Remember that comparing predictions to true labels in the test set can be understood as the unbiased performance evaluation of our model before we let it loose on the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this session, we will use the Wine dataset that is another open-source dataset that is available from the UCI machine learning repository [wine](https://archive.ics.uci.edu/ml/datasets/Wine) it consists of 178 wine samples with 13 features describing their different chemical properties. The samples belong to one of three different classes, $1$, $2$, and $3$, which refer to the three different types of grape grown in the same region in Italy but derived from different wine cultivars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wine data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient way to randomly partition this dataset into separate test and training datasets is to use the train_test_split function from scikit-learn's model_selection submodule. Providing the class label array $y$ as an argument to *stratify* ensures that both training and test datasets have the same class proportions as the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data 70 - 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Features\n",
    "\n",
    "Feature scaling is a crucial step in our preprocessing pipeline that can easily be forgotten. The majority of machine learning and optimization algorithms behave much better if features are on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The importance of feature scaling can be illustrated by a simple example. Let's assume that we have two features where one feature is measured on a scale from 1 to 10 and the second feature is measured on a scale from 1 to 100,000, respectively. When we think of the squared error function in Adaline. t is intuitive to say that the algorithm will mostly be busy optimizing the weights according to the larger errors in the second feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two common approaches to bring different features onto the same scale: **normalization** and **standardization**. **Normalization** refers to the rescaling of the features to a range of $[0, 1]$, which is a special case of min-max scaling. To normalize our data, we can simply apply the min-max scaling to each feature column, where the new value $x_{norm}^{(i)}$ of a sample $x^{(i)}$ can be calculated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{norm}^{(i)} = \\frac{x^{(i)} - x_{min}}{x_{max} - x_{min}}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $x^{(i)}$ is a particular sample, $x_{min}$ is the smallest value in a feature column, and $x_{max}$ the largest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # using the MinMaxScaler class from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although normalization via min-max scaling is a commonly used technique that is useful when we need values in a bounded interval, standardization can be more practical for many machine learning algorithms, especially for optimization algorithms such as gradient descent.\n",
    "\n",
    "The reason is that many linear models, such as the logistic regression and SVM initialize the weights to 0 or small random values close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using standardization, we center the feature columns at mean 0 with standard deviation 1 so that the feature columns have the same parameters as a standard normal distribution (zero mean and unit variance), which makes it easier to learn the weights. Furthermore, standardization maintains useful information about outliers and makes the algorithm less sensitive to them in contrast to min-max scaling, which scales the data to a limited range of values.\n",
    "\n",
    "The procedure for standardization can be expressed by the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "x_{std}^{(i)} = \\frac{x^{(i)} - \\mu_x}{\\sigma_x}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $\\mu_x$ is the sample mean of a particular feature column and $\\sigma_x$ is the corresponding standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table illustrates the difference between *standarization* and *normalizaion*:\n",
    "\n",
    "\n",
    "|Input | Standardized | Min-Max normalized |\n",
    "|:-----|:-------------|:-------------------|\n",
    "|0.0   |-1.46385      |0.0                 |\n",
    "|1.0   |-0.87831      |0.2                 |\n",
    "|2.0   |-0.29277      |0.4                 |\n",
    "|3.0   |0.29277       |0.5                 |\n",
    "|4.0   |0.87831       |0.6                 |\n",
    "|5.0   |1.46385       |1.0                 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization and normalization manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn standardization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting meaningful features\n",
    "\n",
    "If we notice that a model performs much better on a training dataset than on the test dataset, this observation is a strong indicator of **overfitting**. overfitting means the model fits the parameters too closely with regard to the particular observations in the training dataset, but does not generalize well to new data, and we say the model has a **high variance**.  The reason for the overfitting is that our model is too complex for the given training data. Common solutions to reduce the generalization error are listed as follows:\n",
    "* Collect more training data\n",
    "* Introduce a penalty for complexity via regularization\n",
    "* Choose a simpler model with fewer parameters\n",
    "* Reduce the dimensionality of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 and L2 regularization as penalties against model complexity\n",
    "\n",
    "L2 regularization is one approach to reduce the complexity of a model by penalizing large individual weights, where we defined the L2 norm of our weight vector w as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "L2 : \\| w \\|_2^2 = \\sum_{j=1}^m w_j^2\n",
    "\\end{equation*}\n",
    "\n",
    "Another approach to reduce the model complexity is the related L1 regularization:\n",
    "\n",
    "\\begin{equation*}\n",
    "L1: \\| w \\|_1 = \\sum_{j=1}^m \\left| w_j \\right|\n",
    "\\end{equation*}\n",
    "\n",
    "In contrast to L2 regularization, L1 regularization usually yields sparse feature vectors; most feature weights will be zero. Sparsity can be useful in practice if we have a high-dimensional dataset with many features that are irrelevant, especially cases where we have more irrelevant dimensions than samples. In this sense, L1 regularization can be understood as a technique for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric interpretation of L2 regularization\n",
    "\n",
    "As mentioned, L2 regularization adds a penalty term to the cost function that effectively results in less extreme weight values compared to a model trained with an unregularized cost function.\n",
    "\n",
    "For understanding, Let us plot the contours of a convex cost function for two weight coefficients $w_1$ and $w_2$. Here, we will consider the Sum of **Squared Errors (SSE)** cost function that we used for Adaline. Since it is spherical and easier to draw than the cost function of logistic regression; however, the same concepts apply to the latter. Remember that our goal is to find the combination of weight coefficients that minimize the cost function for the training data, as shown in the following figure (the point in the center of the ellipses):\n",
    "\n",
    "<img src=\"images/L2_regularization1.jpg\" alt=\"L2 Regularization\" title=\"L2 Regularization\" height=\"300\" width=\"450\">\n",
    "\n",
    "Now, we can think of regularization as adding a penalty term to the cost function to encourage smaller weights; or in other words, we penalize large weights. Thus, by increasing the regularization strength via the regularization parameter $\\lambda$, we shrink the weights towards zero and decrease the dependence of our model on the training data.\n",
    "\n",
    "<img src=\"images/L2_regularization2.jpg\" alt=\"L2 Regularization\" title=\"L2 Regularization\" height=\"300\" width=\"450\">\n",
    "\n",
    "The quadratic L2 regularization term is represented by the shaded ball. Here, our weight coefficients cannot exceed our regularization budget—the combination of the weight coefficients cannot fall outside the shaded area. On the other hand, we still want to minimize the cost function. Under the penalty constraint, our best effort is to choose the point where the L2 ball intersects with the contours of the unpenalized cost function. The larger the value of the regularization parameter $\\lambda$ gets, the faster the penalized cost grows, which leads to a narrower L2 ball.\n",
    "\n",
    "To summarize the main message of the example, our goal is to minimize the sum of the unpenalized cost plus the penalty term, which can be understood as adding bias and preferring a simpler model to reduce the variance in the absence of sufficient training data to fit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse solutions with L1 regularization\n",
    "\n",
    "he main concept behind L1 regularization is similar to what we have discussed in the previous section. However, since the L1 penalty is the sum of the absolute weight coefficients (remember that the L2 term is quadratic), we can represent it as a diamond-shape budget, as shown in the following figure:\n",
    "\n",
    "<img src=\"images/L1_regularization1.jpg\" alt=\"L2 Regularization\" title=\"L2 Regularization\" height=\"300\" width=\"450\">\n",
    "\n",
    "In the preceding figure, we can see that the contour of the cost function touches the L1 diamond at $w_1 = 0$ . Since the contours of an L1 regularized system are sharp, it is more likely that the optimum—that is, the intersection between the ellipses of the cost function and the boundary of the L1 diamond—is located on the axes, which encourages sparsity.\n",
    "\n",
    "Applied to the standardized Wine data, the L1 regularized logistic regression would yield the following sparse solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LogisticRegression and train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training and test accuracies (both 100 percent) indicate that our model does a perfect job on both datasets. When we access the intercept terms via the lr.intercept_ attribute, we can see that the array returns three values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show te intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we fit the LogisticRegression object on a multiclass dataset, it uses the One-versus-Rest (OvR) approach by default, where the first intercept belongs to the model that fits class 1 versus class 2 and 3, the second value is the intercept of the model that fits class 2 versus class 1 and 3, and the third value is the intercept of the model that fits class 3 versus class 1 and 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight array that we accessed via the lr.coef_ attribute contains three rows of weight coefficients, one weight vector for each class. Each row consists of 13 weights where each weight is multiplied by the respective feature in the 13-dimensional Wine dataset to calculate the net input:\n",
    "\n",
    "\\begin{equation*}\n",
    "z = w_1x_1 + \\dots + w_mx_m = \\sum_{j=0}^m x_jw_j = w^Tx\n",
    "\\end{equation*}\n",
    "\n",
    "> In scikit-learn, $w_0$ corresponds to the *intercept_* and $w_j$ with $j > 0$ correspond to the values in *coef_*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of L1 regularization, which serves as a method for feature selection, we just trained a model that is robust to the potentially irrelevant features in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last example on regularization, we will vary the regularization strength and plot the regularization path—the weight coefficients of the different features for different regularization strengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the weitght coefficient plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot provides us with further insights into the behavior of L1 regularization. As we can see, all feature weights will be zero if we penalize the model with a strong regularization parameter ($C < 0.1$); $C$ is the inverse of the regularization parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential feature selection algorithms\n",
    "\n",
    "An alternative way to reduce the complexity of the model and avoid overfitting is **dimensionality reduction** via feature selection, which is especially useful for unregularized models. There are two main categories of dimensionality reduction techniques: **feature selection** and **feature extraction**. Via feature selection, we select a subset of the original features, whereas in feature extraction, we derive information from the feature set to construct a new feature subspace.\n",
    "\n",
    "Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where $k<d$. The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem.\n",
    "\n",
    "A classic sequential feature selection algorithm is Sequential Backward Selection (SBS), which aims to reduce the dimensionality of the initial feature subspace with a minimum decay in performance of the classifier to improve upon computational efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the SBS algorithm is quite simple: SBS sequentially removes features from the full feature subset until the new feature subspace contains the desired number of features. In order to determine which feature is to be removed at each stage, we need to define the criterion function J that we want to minimize. The criterion calculated by the criterion function can simply be the difference in performance of the classifier before and after the removal of a particular feature. Then, the feature to be removed at each stage can simply be defined as the feature that maximizes this criterion; or in more intuitive terms, at each stage we eliminate the feature that causes the least performance loss after removal. Based on the preceding definition of SBS, we can outline the algorithm in four simple steps:\n",
    "\n",
    "1. Initialize the algorithm with $k=d$, where $d$ is the dimensionality of the full feature space $X_d$.\n",
    "2. Determine the feature $x^{\\_}$ that maximizes the criterion: $x^{\\_} = argmax J \\left( X_k - x \\right)$, where $x \\in X_k$.\n",
    "3. Remove the feature $x^{\\_}$ from the feature set: $X_{k-1} := X_k - x^{\\_}; k:= k - 1$.\n",
    "4. Terminate if $k$ equals the number of desired features; otherwise, go to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go ahead and implement it in Python from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sbs_algorithm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us see our SBS implementation in action using the KNN classifier from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SBS algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SBS fit method will then create new training subsets for testing (validation) and training, which is why this test set is also called the **validation dataset**. This approach is necessary to prevent our **original** test set from becoming part of the training data.\n",
    "\n",
    "Remember that our SBS algorithm collects the scores of the best feature subset at each stage, so let us move on to the more exciting part of our implementation and plot the classification accuracy of the KNN classifier that was calculated on the validation dataset. The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot features vs accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the accuracy of the KNN classifier improved on the validation dataset as we reduced the number of features, which is likely due to a decrease in the curse of dimensionality that we discussed in the context of the KNN algorithm. We can see that the classifier achieved 100 percent accuracy for $k=\\{ 3, 7, 8, 9, 10, 11, 12 \\}$. let's see what the smallest feature subset ($k=3$) that yielded such a good performance on the validation dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's evaluate the performance of the KNN classifier on the original test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us use the selected three-feature subset and see how well KNN performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we did not increase the performance of the KNN model by reducing the number of features, we shrank the size of the dataset, which can be useful in real-world applications that may involve expensive data collection steps. Also, by substantially reducing the number of features, we obtain simpler models, which are easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
