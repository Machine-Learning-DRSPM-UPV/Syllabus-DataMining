{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressing Data via Dimensionality Reduction\n",
    "\n",
    "An alternative approach to feature selection for dimensionality reduction is **feature extraction**. *Data compression* is an important topic in machine learning, and it helps us to store and analyze the increasing amounts of data that are produced and collected in the modern age of technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised dimensionality reduction via principal component analysis\n",
    "\n",
    "Similar to feature selection, we can use different feature extraction techniques to reduce the number of features in a dataset. The difference between feature selection and feature extraction is that while we maintain the original features when we used feature selection algorithms, such as sequential backward selection, we use feature extraction to transform or project the data onto a new feature space. In the context of dimensionality reduction, feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information. \n",
    "\n",
    "Popular applications of PCA include exploratory data analyses and de-noising of signals in stock market trading, and the analysis of genome data and gene expression levels in the field of bioinformatics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "\n",
    "PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one. The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other, as illustrated in the following figure:\n",
    "\n",
    "<img src=\"images/pca_orthogonal.jpeg\" alt=\"PCA orthogonal axes\" title=\"PCA orthogonal axes\" height=\"300\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use PCA for dimensionality reduction, we construct a $d \\times k$–dimensional transformation matrix $W$ that allows us to map a sample vector $x$ onto a new $k$–dimensional feature subspace that has fewer dimensions than the original d–dimensional feature space:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{matrix}\n",
    "x = \\left[ x_1, x_2, \\dots{}, x_d \\right], & x \\in \\mathbb{R}^d \\\\\n",
    "\\downarrow xW, & W \\in \\mathbb{R}^{d \\times k} \\\\\n",
    "z = \\left[ z_1, z_2, \\dots, z_k \\right], & z \\in \\mathbb{R}^k\n",
    "\\end{matrix}\n",
    "\\end{equation*}\n",
    "\n",
    "As a result of transforming the original d-dimensional data onto this new k-dimensional subspace (typically k << d), the first principal component will have the largest possible variance, and all consequent principal components will have the largest variance given the constraint that these components are uncorrelated (orthogonal) to the other principal components—even if the input features are correlated, the resulting principal components will be mutually orthogonal (uncorrelated). Note that the PCA directions are highly sensitive to data scaling, and we need to standardize the features prior to PCA if the features were measured on different scales and we want to assign equal importance to all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's summarize the approach in a few simple steps:\n",
    " \n",
    " 1. Standardize the $d$-dimensional dataset.\n",
    " 2. Construct the covariance matrix.\n",
    " 3. Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    " 4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    " 5. Select $k$ eigenvectors which correspond to the $k$ largest eigenvalues, where $k$ is the dimensionality of the new feature subspace ($k \\leq d$). \n",
    " 6. Construct a projection matrix $W$ from the \"top\" $k$ eigenvectors.\n",
    " 7. Transform the $d$-dimensional input dataset $X$ using the projection matrix $W$ to obtain the new $k$-dimensional feature subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the principal components step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate training and test data (70, 30)\n",
    "\n",
    "# standardize the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the mandatory preprocessing by executing the preceding code, let's advance to the second step: constructing the covariance matrix. The symmetric $d \\times d$-dimensional covariance matrix, where $d$ is the number of dimensions in the dataset, stores the pairwise covariances between the different features. For example, the covariance between two features and on the population level can be calculated via the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sigma_{jk} = \\frac{1}{n} \\sum_{i=1}^n \\left( x_j^{(i)} - \\mu_j \\right) \\left( x_k^{(i)} - \\mu_k \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $\\mu_j$ and $\\mu_k$ are the sample means of features $j$ and $k$, respectively. Note that the sample means are zero if we standardized the dataset. A positive covariance between two features indicates that the features increase or decrease together, whereas a negative covariance indicates that the features vary in opposite directions. For example, the covariance matrix of three features can then be written as follows (note that $Sigma$ stands for the Greek uppercase letter sigma, which is not to be confused with the *sum* symbol):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "\\sigma_1^2 & \\sigma_{12} & \\sigma_{13} \\\\\n",
    "\\sigma_{21} & \\sigma_2^2 & \\sigma_{23} \\\\\n",
    "\\sigma_{31} & \\sigma_{32} & \\sigma_3^2\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "The eigenvectors of the covariance matrix represent the principal components (the directions of maximum variance), whereas the corresponding eigenvalues will define their magnitude. In the case of the Wine dataset, we would obtain 13 eigenvectors and eigenvalues from the $13 \\times 13$-dimensional covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third step, let's obtain the eigenpairs of the covariance matrix. As we remember from our introductory linear algebra classes, an eigenvector v satisfies the following condition:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Sigma{}v = \\lambda{}v\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $\\lambda$ is a scalar: the eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total and explained variance\n",
    "\n",
    "Since we want to reduce the dimensionality of our dataset by compressing it onto a new feature subspace, we only select the subset of the eigenvectors (principal components) that contains most of the information (variance). The eigenvalues define the magnitude of the eigenvectors, so we have to sort the eigenvalues by decreasing magnitude; we are interested in the top k eigenvectors based on the values of their corresponding eigenvalues. But before we collect those $k$ most informative eigenvectors, let us plot the **variance explained** ratios of the eigenvalues. The variance explained ratio of an eigenvalue $\\lambda_j$ is simply the fraction of an eigenvalue $\\lambda_j$ and the total sum of the eigenvalues:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\lambda_j}{\\sum_{j=1}^d \\lambda_j}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with cumsum we can calculate the cumulative sum of expained variances\n",
    "# show the individual and cumulative eplained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variance plot reminds us of the feature importance values that we computed via random forests, we should remind ourselves that PCA is an unsupervised method, which means that information about the class labels is ignored. Whereas a random forest uses the class membership information to compute the node impurities, variance measures the spread of values along a feature axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation\n",
    "\n",
    "Now let's proceed with the last three steps to transform the Wine dataset onto the new principal component axes. The remaining steps we are going to tackle in this section are the following ones:\n",
    "\n",
    "* Select $k$ eigenvectors, which correspond to the $k$ largest eigenvalues, where $k$ is the dimensionality of the new feature subspace ($k \\leq d$). \n",
    "* Construct a projection matrix $W$ from the \"top\" k eigenvectors.\n",
    "* Transform the d-dimensional input dataset $X$ using the projection matrix $W$ to obtain the new $k$-dimensional feature subspace.\n",
    "\n",
    "Or, in less technical terms, we will sort the eigenpairs by descending order of the eigenvalues, construct a projection matrix from the selected eigenvectors, and use the projection matrix to transform the data onto the lower-dimensional subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "\n",
    "# eigen_pairs\n",
    "\n",
    "# sort the (eigenvalue, eigenvector) tuples from high to low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we collect the two eigenvectors that correspond to the two largest eigenvalues, to capture about 60 percent of the variance in this dataset. In practice, the number of principal components has to be determined by a trade-off between computational efficiency and the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we created a 13 x 2-dimensional projection matrix W from top two eigenvectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the projection matrix, we can now transform a sample $x$ (represented as a $1 \\times 13$-dimensional row vector) onto the PCA subspace (the principal components one and two) obtaining $x^{\\prime}$, now a two-dimensional sample vector consisting of two new features:\n",
    "\n",
    "\\begin{equation*}\n",
    "x^{\\prime} = xW\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print x_train_std[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can transform the entire $124 \\times 13$-dimensional training dataset onto the two principal components by calculating the matrix dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the matrix dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les's visualize the transformed Wine training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the resulting plot, the data is more spread along the x-axis—the first principal component—than the second principal component (y-axis), which is consistent with the explained variance ratio plot that we created in the previous subsection. However, we can intuitively see that a linear classifier will likely be able to separate the classes well.\n",
    "\n",
    "**We have to keep in mind that PCA is an unsupervised technique that doesn't use any class label information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis in scikit-learn\n",
    "\n",
    "Now, we will discuss how to use the PCA class implemented in scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PCA from sckit learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the transformed test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in the explained variance ratios of the different principal components, we can simply initialize the PCA class with the n_components parameter set to None, so all principal components are kept and the explained variance ratio can then be accessed via the explained_variance_ratio_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we set n_components=None when we initialized the PCA class so that it will return all principal components in a sorted order instead of performing a dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
