{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised data compression via linear discriminant analysis\n",
    "\n",
    "Linear Discriminant Analysis (LDA) can be used as a technique for feature extraction to increase the computational efficiency and reduce the degree of overfitting due to the curse of dimensionality in non-regularized models.\n",
    "\n",
    "Whereas PCA attempts to find the orthogonal component axes of maximum variance in a dataset, the goal in LDA is to find the feature subspace that optimizes class separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis versus linear discriminant analysis\n",
    "\n",
    "The aim of both is to reduce the number of dimensions in a dataset, PCA is an unsupervised algorithm whereas LDA is supervised. We might think that LDA is superior, however PCA tends to result in better classification results in an image recognition task in certain cases.\n",
    "\n",
    "The following figure summarizes the concept of LDA for a two-class problem.\n",
    "\n",
    "<img src=\"images/lda_concept.jpeg\" alt=\"PCA orthogonal axes\" title=\"PCA orthogonal axes\" height=\"300\" width=\"450\">\n",
    "\n",
    "A linear discriminant, as shown on the x-axis (LD 1), would separate the two normal distributed classes well. Although the exemplary linear discriminant shown on the y-axis (LD 2) captures a lot of the variance in the dataset, it would fail as a good linear discriminant since it does not capture any of the class-discriminatory information.\n",
    "\n",
    "One assumption in LDA is that the data is normally distributed. Also, we assume that the classes have identical covariance matrices and that the samples are statistically independent of each other. However, even if one or more of those assumptions are (slightly) violated, LDA for dimensionality reduction can still work reasonably well "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The inner working\n",
    "\n",
    "Before we dive into the code implementation, let's briefly summarize the main steps that are required to perform LDA:\n",
    "\n",
    "1. Standardize the $d$-dimensional dataset ($d$ is the number of features).\n",
    "2. For each class, compute the $d$-dimensional mean vector.\n",
    "3. Construct the between-class scatter matrix $S_B$ and the within-class scatter matrix $S_w$. \n",
    "4. Compute the eigenvectors and corresponding eigenvalues of the matrix $S_w^{-1} S_B$.\n",
    "5. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "6. Choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues to construct a $d \\times k$-dimensional transformation matrix $W$; the eigenvectors are the columns of this matrix.\n",
    "7. Project the samples onto the new feature subspace using the transformation matrix $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the scatter matrices\n",
    "\n",
    "After load and standardized the features, proceed with the calculation of the mean vectors, which we will use to construct the within-class scatter matrix and between-class scatter matrix, respectively. Each mean vector $m_i$ stores the mean feature value $\\mu_m$ with respect to the samples of class **i**:\n",
    "\n",
    "\\begin{equation*}\n",
    "m_i = \\frac{1}{n_i} \\sum_{x \\in D_i}^c x_m\n",
    "\\end{equation*}\n",
    "\n",
    "This results in three mean vectors:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{matrix}\n",
    "m_i = \\begin{bmatrix}\n",
    "\\mu_i, alcohol \\\\\n",
    "\\mu_i, \\text{malic acid} \\\\\n",
    "\\vdots \\\\\n",
    "\\mu_i, proline\n",
    "\\end{bmatrix} & i \\in \\left\\{ 1, 2, 3 \\right\\} \\\\\n",
    "\\end{matrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate training and test data (70, 30)\n",
    "\n",
    "# standardize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  calculates and displays the mean vector for each of the three classes in the standardized training datase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mean vectors, we can now compute the within-class scatter matrix $S_w$:\n",
    "\n",
    "\\begin{equation*}\n",
    "S_w = \\sum_{i=1}^c S_i\n",
    "\\end{equation*}\n",
    "\n",
    "This is calculated by summing up the individual scatter matrices $S_i$ of each individual class $i$:\n",
    "\n",
    "\\begin{equation*}\n",
    "S_i = \\sum_{x \\in D_i}^c \\left( x - m_i \\right) \\left( x - m_i \\right)^T\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterates through classes, calculates the scatter matrix for each, and accumulates them into the within-class scatter matrix (S_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption that we are making when we are computing the scatter matrices is that the class labels in the training set are uniformly distributed. However, if we print the number of class labels, we see that this assumption is violated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we want to scale the individual scatter matrices $S_i$ before we sum them up as scatter matrix $S_w$. When we divide the scatter matrices by the number of class-samples $n_i$, we can see that computing the scatter matrix is in fact the same as computing the covariance matrix $\\Sigma_i$ —the covariance matrix is a normalized version of the scatter matrix:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Sigma_i = \\frac{1}{n_i}S_w = \\frac{1}{n_i} \\sum_{x \\in D_i}^c \\left( x - m_i \\right) \\left( x - m_i \\right)^T\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the within-class scatter matrix (S_W) by accumulating the covariance matrices for each class in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we computed the scaled within-class scatter matrix (or covariance matrix), we can move on to the next step and compute the between-class scatter matrix $S_B$:\n",
    "\n",
    "\\begin{equation*}\n",
    "S_B = \\sum_{i=1}^c n_i \\left( m_i - m \\right) \\left( m_i - m \\right)^T\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $m$ is the overall mean that is computed, including samples from all classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the between-class scatter matrix (S_B) by iterating through class means, considering class sizes, and accumulating the weighted scatter matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting linear discriminants for the new feature subspace\n",
    "\n",
    "The remaining steps of the LDA are similar to the steps of the PCA. However, instead of performing the eigendecomposition on the covariance matrix, we solve the generalized eigenvalue problem of the matrix $S_w^{-1} S_B$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate eigenvalues and eigenvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the eigenvalues in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LDA, the number of linear discriminants is at most $c−1$, where c is the number of class labels, since the in-between scatter matrix $S_B$ is the sum of c matrices with rank 1 or less. We can indeed see that we only have two nonzero eigenvalues (the eigenvalues 3-14 are not exactly zero, but this is due to the floating point arithmetic in NumPy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure how much of the class-discriminatory information is captured by the linear discriminants (eigenvectors), let's plot the linear discriminants by decreasing eigenvalues similar to the explained variance plot that we created in the PCA section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show individual and cumulative discrimilability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now stack the two most discriminative eigenvector columns to create the transformation matrix $W$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting samples onto the new feature space\n",
    "\n",
    "Using the transformation matrix $W$ that we created in the previous subsection, we can now transform the training dataset by multiplying the matrices:\n",
    "\n",
    "\\begin{equation*}\n",
    "X^{\\prime} = XW\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projecting samples onto the new feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA via scikit-learn\n",
    "\n",
    "Now, let's look at the LDA class implemented in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA via scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ler's see how it works with logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By lowering the regularization strength, we could probably shift the decision boundaries so that the logistic regression model classifies all samples in the training dataset correctly. However, and more importantly, let us take a look at the results on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
